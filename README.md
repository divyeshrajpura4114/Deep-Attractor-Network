# Deep Attractor Netowrk for Music Source Separation
____

This repository is implemented for Music Source Separation on MUSDB-18 database. Currently, it focuses on spearation into two sources, namely, vocal and instrument mixture. However, it can be easily modified to have multiple sources. 

## Directory Structure:
```
deep-attractor-network
|
└─── data
|   |   
|   └─── musdb18_train
|   |   |   wav.scp
|   |   |   utt2dur
|   |   |   segments
|   |   |   feats.scp
|   |  
|   └─── musdb18_test
|       |   wav.scp
|       |   utt2dur
|       |   segments
|       |   feats.scp 
|    
└─── local
|   |   nnet
|   |   common
|
└─── exp
|   |   
|   └─── musdb18_train
|   |   |   ckpt_models
|   |  
|   └─── musdb18_test
|       |   result
|
└─── mfcc
|
└─── run_danet.sh

```
Data Dircoretory:
Data directory is collection of various files that contains information about path and duration of utterance. If you are falimiar with the Data Preparation in the Kaldi, you can skip this section. 
1. wav.scp
    This file is generated by local/make_musdb18.pl. The format of this file is 
        ```
        <utt-id> <utt-path>
        ```
        where the <utt-id> is uniq identifier for utternace and <utt-path> is the corresponding path of utterance.

2. utt2dur
    This file is generated from local/get_utt2dur.py. The format of this file is 
        ```
        <utt-id> <utt-dur>
        ```
        where the <utt-dur> is duration of the utterance.

3. segments
    This file is generated from local/get_segments.py. The script local/get_segments.py split the utterance into smaller segments of duration 1.0s with overlapping of 0.5s. The duration and overlapping for segment generation can be changed from conf/conf.yaml file. The format of this file is 
        ```
        <segment-id> <utt-id> <segment-begin> <segment-end>
        ```
        where the <segment-begin> and <segment-end> are start and end duration of segment (measured in seconds). <segment-id> is uniq identifier for segment and given by <utt-id>_<segment-begin>_<segment-end>.

4. feats.scp
    This file is generated during feature extraction. The format of this file is 
        ```
        <segment-id> <feat-path>
        ```
        where the <feat-path> is path of features for segment <segment-id>.


## Prerequisites:

- Linux, MacOS
- Python 3
- CPU or NVIDIA GPU + CUDA CuDNN
- ffmpeg (Command line utility that can convert various formats of computer audio files in to other formats.)

## Required Python libraries:

```
torch==1.3.1
torchvision==0.1.8.8
librosa==0.8.0
visdom==0.1.8.9
matplotlib==3.3.0
numpy==1.19.1
scipy==1.5.2
sklearn=0.23.2
dotmap==1.3.17
```
These libraries can be installed via following command:

```
pip install -r requirements.txt
```

## Task to be done
- [ ] Add evaluation script
- [ ] Modify visdom plotting
- [ ] Verify script for multiple (more than 2) sources
- [ ] Add Support to separate the sources from full length utterance

```diff
- Note: This repository is modified version of the original implementation provided at https://github.com/naplab/DANet by authors  
```

# Citation:

If you use this code for your research, please cite below papers.

```

@ARTICLE{YiLuo2018IEEETransASLP,
    author={Y. {Luo} and Z. {Chen} and N. {Mesgarani}},
    journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
    title={Speaker-Independent Speech Separation With Deep Attractor Network},
    year={Apr, 2018},
    volume={26},
    number={4},
    pages={787-796},
    doi={10.1109/TASLP.2018.2795749},
    ISSN={2329-9304},
}
@INPROCEEDINGS{ZhuoChen2017ICASSP, 
    author={Z. {Chen} and Y. {Luo} and N. {Mesgarani}}, 
    booktitle={2017 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}, 
    title={Deep attractor network for single-microphone speaker separation}, 
    address={New Orleans, Louisiana, USA},
    year={Mar 05-09, 2017},
    volume={}, 
    number={}, 
    pages={246-250}, 
    doi={10.1109/ICASSP.2017.7952155}, 
    ISSN={2379-190X}, 
}

@inproceedings{RajathKumar2018Interspeech,
    author={Rajath Kumar and Yi Luo and Nima Mesgarani},
    title={Music Source Activity Detection and Separation Using Deep Attractor Network},
    Address = {Hyderabad, India},
    year={Sep 02-06, 2018},
    booktitle={Proc. INTERSPEECH 2018},
    pages={347--351},
    doi={10.21437/Interspeech.2018-2326},
}

@INPROCEEDINGS{DivyeshRajpura2020SPCOM,
    author={Divyesh G. Rajpura and Jui Shah and Maitreya Patel and Harshit Malaviya and Kirtana Phatnani and Hemant A. Patil},
    booktitle={accepted in 2020 International Conference on Signal Processing and Communications (SPCOM)}, 
    title={Effectiveness of Transfer Learning on Singing Voice Conversion in the Presence of Background Music}, 
    address={Bengaluru, India},
    year={Jul 20-23, 2020},
    volume={},
    number={},
    pages={1-5},
}
```